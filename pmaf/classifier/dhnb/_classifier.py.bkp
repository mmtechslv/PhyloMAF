from ._base import DHNBClassifierBase
from pmaf.classifier._metakit import ClassifierBaseMetabase,ClassifierDatasetMetabase
from pmaf.database._metakit import DatabaseBackboneMetabase
from collections import defaultdict
from h5netcdf import File as H5NCDF_File
from pmaf.internal._extensions._cpython._pmafc_extension import make_sequence_record_tuple
from pmaf.internal._shared import chunk_generator
from pmaf.classifier.dhnb._shared import adjust_chunksize,add_primer_prefix, optimize_secondary_chunksize, calc_optimal_chunksize
from pmaf.classifier.dhnb._constructors import parse_refine_primer_chunk, generate_theoretical_hmers,parse_chunk_shelve_digest,\
    parse_process_chunk,verify_kmer_sizes, db_chunk_record_generator
import joblib as jbl
import xarray as xr
import warnings
warnings.simplefilter('ignore', category=UserWarning)
from math import ceil
import numpy as np
from os import path
from datetime import datetime
from tempfile import mkdtemp
from dask import delayed
import dask.array as da
from dask.diagnostics import ProgressBar
from progress.bar import Bar
from tabulate import tabulate
from tempfile import TemporaryDirectory
from sys import stdout
Bar.check_tty = False
Bar.file = stdout


class DHNBClassifier(DHNBClassifierBase,ClassifierBaseMetabase):
    __cache_prefix = 'cbrp_'
    __cache_build_prefix = 'build_cbrp_'
    def __init__(self, classifier_netcdf_fp, database_instance , cache_dir_fp=None, **kwargs):
        if not isinstance(database_instance, DatabaseBackboneMetabase):
            raise TypeError('`database_instance` has invalid type.')
        max_seq_length = database_instance.get_stats_by_rid(include='length').max()
        super().__init__(classifier_netcdf_fp, cache_dir_fp, 'cls_kmer_sizes', 'cls_compression',max_seq_length, **kwargs)
        if self.xr.cls_database_type != type(database_instance).__name__:
            raise TypeError('Database is not compatible with classifier.')
        self._db_instance = database_instance
        self._db_database_type = type(database_instance)
        self._index_tid = self.xr.coords['tid'].to_index()
        self._tmeta =  self.xr['tmeta']
        self._index_mtitem = None
        self._valid_tids = None
        self._invalid_tids = None
        self._reset_inits()


    def chunk(self, *args, **kwargs):
        if self.state:
            if not self.chunked:
                if len(kwargs)>0:
                    if 'tid' in kwargs.keys():
                        tid_chksz = kwargs['tid']
                    else:
                        tid_chksz = len(self._index_tid)
                else:
                    if len(args)==1:
                        tid_chksz = args[0]
                    else:
                        raise ValueError('Invalid chunk sizes are provided.')
                
                super().chunk(*args, **kwargs)
                self._tmeta = self.xr['tmeta'].chunk({'tid': tid_chksz})
            else:
                raise RuntimeError('Data is already chunked.')
        else:
            raise RuntimeError('Basefile is closed or at hold')

    def unchunk(self):
        if self.state:
            if self.chunked:
                self._tmeta = self.xr['tmeta']
                super().unchunk()
            else:
                raise RuntimeError('Data is already unchunked.')
        else:
            raise RuntimeError('Basefile is closed or at hold')

    def _reset_inits(self):
        super()._reset_inits()
        tmp_valid_tid_map = self.xr['tmeta'].to_pandas()['valid']
        self._valid_tids = tmp_valid_tid_map[tmp_valid_tid_map.astype(np.bool_)].index.values
        self._invalid_tids = tmp_valid_tid_map[~tmp_valid_tid_map.astype(np.bool_)].index.values
        self._index_mtitem = self.xr.coords['mtitem'].to_index()

    @classmethod
    def verify_dataset(self, dataset_instance):
        ret = False
        if (isinstance(dataset_instance, ClassifierDatasetMetabase) and isinstance(dataset_instance, DHNBClassifierBase)):
            ret = True
        return ret

    @classmethod
    def verify_database(self, database_instance):
        ret = False
        if isinstance(database_instance, DatabaseBackboneMetabase):
            if database_instance.storage_manager.state == 1:
                if database_instance.storage_manager.has_repseq:
                    ret = True
        return ret

    @classmethod
    def verify_basefile(cls, basefile_fp):
        ret = False
        if isinstance(basefile_fp, str):
            if path.exists(basefile_fp):
                dataset_xr = xr.open_dataset(basefile_fp,engine='h5netcdf')
                if dataset_xr.cls_type == cls.__name__:
                    ret = True
                dataset_xr.close()
        return ret
    
    @classmethod
    def _filter_by_ksize(cls,database_instance,kmer_size):
        rstat_N_repmin = database_instance.get_stats_by_rid(include='N_repmax')
        tmp_tid_stats = database_instance.get_stats_by_tid(include=['singleton', 'subseqs'])
        non_singleton_tid_subs = tmp_tid_stats[~tmp_tid_stats['singleton']]['subseqs']
        higly_N_rep_rids = rstat_N_repmin[rstat_N_repmin >= kmer_size].index
        pre_potential_exc_tids = database_instance.find_tid_by_rid(higly_N_rep_rids,flatten=True,mode='array')
        potential_exc_tids = non_singleton_tid_subs.index[non_singleton_tid_subs.index.isin(pre_potential_exc_tids)]
        if len(potential_exc_tids)>0:
            potential_exc_tids_rids_sum = database_instance.find_rid_by_tid(potential_exc_tids,subs=True,mode='frame').map(lambda rids:np.isin(rids,higly_N_rep_rids).sum())
            possible_singleton_tids = non_singleton_tid_subs.loc[potential_exc_tids].sub(potential_exc_tids_rids_sum)
            new_singleton_tids = possible_singleton_tids[possible_singleton_tids<2].unique()
            return higly_N_rep_rids.values, new_singleton_tids
        else:
            return np.asarray([]),np.asarray([])

    def __filter_by_primer(self, primers_dict):
        verified_primers_dict = defaultdict(dict)
        rstat_N_repmin = self.database.get_stats_by_rid(include='N_repmax')
        tmp_tid_stats = self.database.get_stats_by_tid(include=['singleton', 'subseqs'])
        non_singleton_subs = tmp_tid_stats[~tmp_tid_stats['singleton']]['subseqs']
        for label, primer_sequence in primers_dict.items():
            primer_exc_rids = rstat_N_repmin[rstat_N_repmin >= len(primer_sequence)].index
            primer_exc_tids = self.database.find_tid_by_rid(primer_exc_rids,flatten=True,mode='array')
            if len(primer_exc_tids)>0:
                primer_exc_tid_exc_rid_totals = self.database.find_rid_by_tid(primer_exc_tids,subs=True,mode='frame').map(lambda rids:np.isin(rids,primer_exc_rids).sum())
                possible_singleton_tids = non_singleton_subs.loc[primer_exc_tids].sub(primer_exc_tid_exc_rid_totals)
                new_singleton_tids = possible_singleton_tids[possible_singleton_tids<2].unique()
                verified_primers_dict[label] = {'rids':primer_exc_rids.values,'tids':new_singleton_tids}
            else:
                verified_primers_dict[label] = {'rids':np.asarray([]),'tids':np.asarray([])}
        return dict(verified_primers_dict)

    def __init_primer_metadata(self, primers_dict):

        prior_rmeta_var = self.xr['rmeta']
        prior_tmeta_var = self.xr['tmeta']

        new_mritem_list = []
        new_mtitem_list = []
        for primer_label, primer_seq_str in primers_dict.items():
            if add_primer_prefix(primer_label, 'valid') not in self.xmritem:
                new_mritem_list.append(add_primer_prefix(primer_label, 'state'))
                new_mritem_list.append(add_primer_prefix(primer_label, 'valid'))
                new_mritem_list.append(add_primer_prefix(primer_label, 'first-pos'))
                new_mtitem_list.append(add_primer_prefix(primer_label, 'valid'))

        if len(new_mritem_list) > 0:
            new_mritem_coords = np.asarray(new_mritem_list, dtype=object)
            new_rmeta_var = xr.DataArray(data=np.zeros((len(self.xrid), len(new_mritem_coords)), dtype='u4'), dims=('rid', 'mritem'), coords={'rid': self.xrid, 'mritem': new_mritem_coords})
            post_rmeta_var = xr.concat((prior_rmeta_var, new_rmeta_var), dim='mritem', join='exact')
        else:
            post_rmeta_var = prior_rmeta_var.load()

        if len(new_mtitem_list) > 0:
            new_mtitem_coords = np.asarray(new_mtitem_list, dtype=object)
            new_tmeta_var = xr.DataArray(data=np.zeros((len(self.xtid), len(new_mtitem_coords)), dtype='u4'), dims=('tid', 'mtitem'), coords={'tid': self.xtid, 'mtitem': new_mtitem_coords})
            post_tmeta_var = xr.concat((prior_tmeta_var, new_tmeta_var), dim='mtitem', join='exact')
        else:
            post_tmeta_var = prior_tmeta_var.load()

        primer_verified_dict = self.__filter_by_primer(primers_dict)

        for primer_label, verification_result in primer_verified_dict.items():
            primer_passed_rid_mask = (~self.xrid.isin(verification_result['rids']))
            primer_passed_tid_mask = (~self.xtid.isin(verification_result['tids']))
            post_rmeta_var.loc[primer_passed_rid_mask, add_primer_prefix(primer_label, 'valid')] = 1
            post_tmeta_var.loc[primer_passed_tid_mask, add_primer_prefix(primer_label, 'valid')] = 1

        return post_rmeta_var, post_tmeta_var

    def _refine_by_primer(self, primers_dict):

        if not isinstance(primers_dict, dict):
            raise TypeError('`primer_dict` must be a dictionary.')
        if self.cache_dir is not None:
            cache_dir = mkdtemp(prefix=self.__cache_prefix, dir=self.cache_dir)
            tmp_dir = None
        else:
            tmp_dir = TemporaryDirectory(prefix=self.__cache_prefix)
            cache_dir = tmp_dir.name

        print("> Parsing primers and assigning metadata.")

        rmeta_var, tmeta_var = self.__init_primer_metadata(primers_dict)

        all_valid_label = ['valid'] + [add_primer_prefix(primer_label, 'valid') for primer_label in primers_dict.keys()]

        valid_rid_vector = rmeta_var.indexes['rid'][rmeta_var.loc[:, all_valid_label].all(dim='mritem')]
        valid_tid_vector = tmeta_var.indexes['tid'][tmeta_var.loc[:, all_valid_label].all(dim='mtitem')]

        primer_records_list = []
        ordered_primer_ids = []
        ordered_mritem_labels = []

        for primer_i, (primer_label, primer_seq_str) in enumerate(primers_dict.items()):
            primer_id = primer_i + 1
            ordered_primer_ids.append(primer_id)
            primer_records_list.append(make_sequence_record_tuple(primer_id, primer_seq_str))
            state_label = add_primer_prefix(primer_label, 'state')
            first_post_label = add_primer_prefix(primer_label, 'first-pos')
            ordered_mritem_labels.extend([state_label, first_post_label])

        primer_records_tuple = tuple(primer_records_list)

        rid_empc_levels = rmeta_var.loc[valid_rid_vector, 'empc'].to_pandas()

        print("> Making preparations to get best performance.")

        refiner_memory_cache = jbl.Memory(location=cache_dir, mmap_mode='r', verbose=0)

        parse_refine_primer_chunk_cached = refiner_memory_cache.cache(parse_refine_primer_chunk)

        parsed_chunk_result_shelves = []

        print('> Parsing and caching reference database sequences.')
        db_digest_pbar = Bar('Digesting Sequences', max=len(valid_rid_vector))

        for d_empc_level, d_rid_group in rid_empc_levels.groupby(rid_empc_levels, sort=True):
            d_io_batch_size, d_exec_chunksize = adjust_chunksize(self.io_chunksize, self.exec_chunksize, d_empc_level, d_rid_group.index.shape[0], self.nworkers)
            d_rid_list = d_rid_group.index.tolist()
            if len(d_rid_list):
                db_digest_pbar.message = 'Digesting Sequence Difficulty Level - {}:'.format(d_empc_level)
            for d_r_rid_list in chunk_generator(d_rid_list, self.run_chunksize):
                parsed_chunk_result_shelves.extend(
                    jbl.Parallel(n_jobs=self.nworkers, prefer='threads', batch_size=d_io_batch_size, pre_dispatch=2 * d_io_batch_size, verbose=0)
                    (jbl.delayed(lambda *args: (args[0], parse_refine_primer_chunk_cached.call_and_shelve(*args)))
                     (d_exec_chunk_rids, ordered_primer_ids, d_exec_chunk_record_tuples, primer_records_tuple)
                     for d_exec_chunk_rids, d_exec_chunk_record_tuples in db_chunk_record_generator(d_r_rid_list, d_exec_chunksize, self.database, db_digest_pbar)))
        db_digest_pbar.finish()

        print("\n> Finalizing Refining.")
        db_write_pbar = Bar('Building dataset from digest.', max=len(valid_rid_vector))
        for rid_chunk, parsed_chunk_result_shelve in parsed_chunk_result_shelves:
            parsed_result = parsed_chunk_result_shelve.get()
            rmeta_var.loc[list(rid_chunk), ordered_mritem_labels] = parsed_result
            db_write_pbar.next(len(rid_chunk))

        print("\n> Adding digest into storage file.")
        self._hold_basefile()
        tmp_store = H5NCDF_File(self.basefile_fp, mode='a')
        tmp_store.resize_dimension('mritem', len(rmeta_var.coords['mritem']))
        tmp_store.resize_dimension('mtitem', len(tmeta_var.coords['mtitem']))
        tmp_store['mritem'][:] = rmeta_var.coords['mritem']
        tmp_store['mtitem'][:] = tmeta_var.coords['mtitem']
        tmp_store['rmeta'][:] = rmeta_var.values
        tmp_store['tmeta'][:] = tmeta_var.values
        tmp_store.sync()
        tmp_store.close()
        self._release_basefile()
        self._reset_inits()
        refiner_memory_cache.clear(warn=False)
        if self.cache_dir is None:
            tmp_dir.cleanup()
        return

    @classmethod
    def build(cls, database_instance, classifier_fp,
              kmer_sizes=(6, 7),
              cache_dir_fp=None,
              run_chunksize=30000, io_chunksize=None, exec_chunksize=None,
              empc_cutoff=35,
              nworkers=(8,16),
              force_new=False,
              compress=4,
              memmap=True,
              author='', name=''):

        if not isinstance(database_instance, DatabaseBackboneMetabase):
            raise TypeError('`database_instance` has invalid type.')
        else:
            if database_instance.storage_manager.state != 1:
                raise ValueError('`database_instance` has invalid state.')
            else:
                if not database_instance.storage_manager.has_repseq:
                    raise ValueError('`database_instance` does not have any representative sequences.')
        if not isinstance(nworkers, tuple):
            raise TypeError('`nworkers` must be `tuple`.')
        if not all([isinstance(n, int) for n in nworkers]):
            raise TypeError('`nworkers` must be (`int`, `int`).')
        if not isinstance(author, str):
            raise TypeError('`author` must have `str` type.')
        if not isinstance(name, str):
            raise TypeError('`name` must have `str` type.')
        if not isinstance(compress, int):
            raise TypeError('`compress` must be integer.')
        else:
            if (compress > 9) and (compress < 0):
                raise ValueError('`compress` can be greater or equal to 0 and less than 9.')
        if (path.exists(classifier_fp)) and (not force_new):
            raise FileExistsError('File must not exist. Use `force_new` = True to rewrite.')
        if cache_dir_fp is not None:
            if not path.isdir(cache_dir_fp):
                raise NotADirectoryError('`cache_dir_fp` is invalid.')
        if empc_cutoff <= 1:
            raise ValueError('`empc_cutoff must be greater than 1.`')
        if not ((cls.MEM_LIMIT > 1) and (cls.MEM_LIMIT < 40)):
            raise ValueError('`mem_limit` must be greater than 1% and less than 40%.`')
        if not isinstance(kmer_sizes, tuple):
            raise TypeError('`kmer_sizes` must be tuple.')
        else:
            if not verify_kmer_sizes(kmer_sizes):
                raise ValueError('`kmer_sizes` have invalid combination.')
        if len(kmer_sizes) == 0:
            raise ValueError('`kmer_sizes` cannot be empty.')
        if (io_chunksize is None) or (exec_chunksize is None):
            rid_length_stats = database_instance.get_stats_by_rid(include='length')
            io_chunksize, exec_chunksize = optimize_secondary_chunksize(run_chunksize, nworkers[0], max(kmer_sizes), int(rid_length_stats.max()), True, cls.MEM_LIMIT, True)
        else:
            if not cls._verify_chunksize(run_chunksize, io_chunksize, exec_chunksize):
                raise ValueError('Chunk parameters are invalid. Follow the rule `run_chunksize` > `io_chunksize` > `exec_chunksize`.')
        if cache_dir_fp is not None:
            cache_dir = mkdtemp(prefix=cls.__cache_build_prefix, dir=cache_dir_fp)
            tmp_dir = None
        else:
            tmp_dir = TemporaryDirectory(prefix=cls.__cache_build_prefix)
            cache_dir = tmp_dir.name
        zlib_state = True if compress > 0 else False
        cmp_level = compress if compress > 0 else 4
        
        print("> Making preparations to get best performance.\n")
        if memmap:
            parser_memory_cache = jbl.Memory(location=cache_dir, mmap_mode='r', verbose=0)
        else:
            parser_memory_cache = jbl.Memory(location=cache_dir, verbose=0, compress=ceil(cmp_level / 2) if zlib_state else False)
        
        db_name = database_instance.name
        db_type = type(database_instance).__name__
        repseq_stats = database_instance.get_stats_by_rid(include=cls.EMPC_FACTORS)
        tid_stats_singeltons = database_instance.get_stats_by_tid(include='singleton')
        prior_tid_singeltons = tid_stats_singeltons[tid_stats_singeltons].index.values
        prior_rid_tid_singeltons = database_instance.find_rid_by_tid(prior_tid_singeltons, flatten=True, mode='array')
        new_rid_singelton, new_tid_singeltons = cls._filter_by_ksize(database_instance, min(kmer_sizes))
        pre_excluded_rids = np.union1d(prior_rid_tid_singeltons, new_rid_singelton)
        tid_singeltons = np.union1d(prior_tid_singeltons, new_tid_singeltons)
        included_rids_array, excluded_rids_array, difficulty_chunks = cls._split_by_difficulty_chunks(repseq_stats, kmer_sizes, io_chunksize, exec_chunksize, empc_cutoff, nworkers[0], pre_excluded_rids)
        
        all_tids_vector = np.sort(database_instance.xtid.values)
        all_hmer_vector = np.sort(np.array(list(generate_theoretical_hmers(kmer_sizes, io_chunksize)), dtype=np.uint64))
        total_rids = len(included_rids_array) + len(excluded_rids_array)
        total_hmers = len(all_hmer_vector)
        total_tids = len(all_tids_vector)
        
        current_timestamp = datetime.now()
        
        hmer_coord = all_hmer_vector.astype('u8')
        tid_coord = all_tids_vector.astype('u4')
        mritem_coord = np.asarray(['valid', 'empc'], dtype=str)
        mtitem_coord = np.asarray(['valid'], dtype=str)
        mhitem_coord = np.asarray(['fp', 'lp'], dtype=str)
        
        raw_chunksize = calc_optimal_chunksize(1, exec_chunksize, total_rids, total_hmers, None, cls.MEM_LIMIT)
        rmeta_chunksize = calc_optimal_chunksize(4, exec_chunksize, total_rids, 2, None, cls.MEM_LIMIT)
        tmeta_chunksize = (total_tids,1)
        hmeta_chunksize = calc_optimal_chunksize(2, exec_chunksize, total_rids, total_hmers, 2, cls.MEM_LIMIT)
        
        optimization_summary = [['Samples/Features', total_rids, total_tids, total_hmers],
                                ['Singletons', len(pre_excluded_rids), len(tid_singeltons), None],
                                ['Passed', len(included_rids_array), total_tids - len(tid_singeltons), None],
                                ['Excluded', len(excluded_rids_array), len(tid_singeltons), None],
                                ['Difficulty Level 1', len(difficulty_chunks[0][2]), None, None],
                                ['Difficulty Level 2', len(difficulty_chunks[1][2]), None, None],
                                ['Difficulty Level 3', len(difficulty_chunks[2][2]), None, None],
                                ['`raw` chunksize', raw_chunksize[0], None, raw_chunksize[1]],
                                ['`rmeta` chunksize', rmeta_chunksize[0], None, None],
                                ['`tmeta` chunksize', None, tmeta_chunksize[0], None],
                                ['`hmeta` chunksize', hmeta_chunksize[0], None, hmeta_chunksize[1]]]
        
        print(tabulate(optimization_summary, headers=['Total', 'Refs (Seq)', 'Reps (Tax)', 'K-mers'], tablefmt='github', numalign="center", missingval='-'))
        
        estimated_filesize = (total_rids * (1 * total_hmers + 4 * 2 + 2 * total_hmers * 2) + 1 * total_tids * 4) / (1024 ** 3)
        
        print("\nEstimated Uncompressed File Size: {:,.5g} Gigabytes".format(estimated_filesize))
        print("Approximate Compressed File Size: {:,.3g} Gigabytes".format(estimated_filesize * 0.05))
        print("\nRuntime Chunksize: {:,} per chunk.".format(run_chunksize))
        print("I/O Chunksize: {:,} per chunk/batch.".format(io_chunksize))
        print("Execution Chunksize: {:,} per chunk.".format(exec_chunksize))
        
        parse_process_chunk_cached = parser_memory_cache.cache(parse_process_chunk)
        parsed_chunk_result_shelves = []
        
        print('\n> Parsing and caching reference database sequences.')
        db_digest_pbar = Bar('Digesting Sequences', max=len(included_rids_array))
        
        for d_io_batch_size, d_exec_chunksize, d_rid_list, d_empc_level in difficulty_chunks:
            if len(d_rid_list):
                db_digest_pbar.message = 'Digesting Sequence Difficulty Level - {}:'.format(d_empc_level)
            for d_r_rid_list in chunk_generator(d_rid_list, run_chunksize):
                parsed_chunk_result_shelves.extend(jbl.Parallel(n_jobs=nworkers[0], prefer='processes', batch_size=d_io_batch_size, pre_dispatch=2 * d_io_batch_size, verbose=0) \
                                                       (jbl.delayed(lambda *args: (args[0], parse_process_chunk_cached.call_and_shelve(*args)))
                                                        (d_exec_chunk_rids, d_exec_chunk_record_tuples, d_empc_level, kmer_sizes)
                                                        for d_exec_chunk_rids, d_exec_chunk_record_tuples in db_chunk_record_generator(d_r_rid_list, d_exec_chunksize, database_instance, db_digest_pbar)))
        
        db_digest_pbar.finish()
        
        print("\n> Preparing Dask task graph.")
        
        raw_blocks_list = []
        rmeta_blocks_list = []
        hmeta_blocks_list = []
        
        tmp_raw_blocks_list = []
        tmp_rmeta_blocks_list = []
        tmp_hmeta_blocks_list = []
        
        raw_rid_chunksize = round(raw_chunksize[0]/exec_chunksize)
        rmeta_rid_chunksize = round(rmeta_chunksize[0]/exec_chunksize)
        hmeta_rid_chunksize = round(hmeta_chunksize[0]/exec_chunksize)
        
        rid_block_chunks_list = []
        rid_ordered_list = []
        
        all_hmer_vector_delayed = delayed(all_hmer_vector)
        
        for block_i, (rid_chunk, parsed_chunk_result_shelve) in enumerate(parsed_chunk_result_shelves):
            rid_ordered_list.extend(rid_chunk)
            block_chunksize = len(rid_chunk)
            raw_block_shape = (block_chunksize, total_hmers)
            rmeta_block_shape = (block_chunksize, 2)
            hmeta_block_shape = (block_chunksize, total_hmers, 2)
            rid_block_chunks_list.append(block_chunksize)
        
            raw_block_matrix, rmeta_block_matrix, hmeta_block_matrix = parse_chunk_shelve_digest(block_chunksize, all_hmer_vector_delayed, parsed_chunk_result_shelve)
        
            raw_block_da = da.from_delayed(raw_block_matrix, shape=raw_block_shape, dtype='b').rechunk({1:raw_chunksize[1]})
            rmeta_block_da = da.from_delayed(rmeta_block_matrix, shape=rmeta_block_shape, dtype='u4').rechunk({1:rmeta_chunksize[1]})
            hmeta_block_da = da.from_delayed(hmeta_block_matrix, shape=hmeta_block_shape, dtype='u2').rechunk({1:hmeta_chunksize[1]})
           
            tmp_raw_blocks_list.append(raw_block_da)
            tmp_rmeta_blocks_list.append(rmeta_block_da)
            tmp_hmeta_blocks_list.append(hmeta_block_da)
        
        for raw_new_chunk in chunk_generator(tmp_raw_blocks_list,raw_rid_chunksize):
            raw_blocks_list.append(da.concatenate(raw_new_chunk, axis=0).rechunk({0:raw_chunksize[0]}))
        for rmeta_new_chunk in chunk_generator(tmp_rmeta_blocks_list,rmeta_rid_chunksize):
            rmeta_blocks_list.append(da.concatenate(rmeta_new_chunk, axis=0).rechunk({0:rmeta_chunksize[0]}))
        for hmeta_new_chunk in chunk_generator(tmp_hmeta_blocks_list,hmeta_rid_chunksize):
            hmeta_blocks_list.append(da.concatenate(hmeta_new_chunk, axis=0).rechunk({0:hmeta_chunksize[0]}))
        
        rid_ordered_list.extend(excluded_rids_array.tolist())
        rid_block_chunks_list.append(len(excluded_rids_array))
        
        raw_excluded_da = da.zeros((len(excluded_rids_array), total_hmers), dtype='b', chunks=raw_chunksize)
        rmeta_excluded_da = da.zeros((len(excluded_rids_array), 2), dtype='u4', chunks=rmeta_chunksize)
        hmeta_excluded_da = da.zeros((len(excluded_rids_array), total_hmers, 2), dtype='u2', chunks=hmeta_chunksize)
        
        raw_blocks_list.append(raw_excluded_da)
        rmeta_blocks_list.append(rmeta_excluded_da)
        hmeta_blocks_list.append(hmeta_excluded_da)
        
        rid_coord = np.asarray(rid_ordered_list, dtype='u4')
        
        singeltons_mask = da.isin(all_tids_vector, tid_singeltons, assume_unique=True)
        tmeta_array = da.where(singeltons_mask, 0, 1).astype('u4').reshape((len(all_tids_vector), 1))
        
        raw_var = xr.DataArray(data=da.concatenate(raw_blocks_list, axis=0), dims=('rid', 'hmer'), coords={'rid': rid_coord, 'hmer': hmer_coord})
        rmeta_var = xr.DataArray(data=da.concatenate(rmeta_blocks_list, axis=0), dims=('rid', 'mritem'), coords={'rid': rid_coord, 'mritem': mritem_coord})
        hmeta_var = xr.DataArray(data=da.concatenate(hmeta_blocks_list, axis=0), dims=('rid', 'hmer', 'mhitem'), coords={'rid': rid_coord, 'hmer': hmer_coord, 'mhitem': mhitem_coord})
        tmeta_var = xr.DataArray(data=tmeta_array, dims=('tid', 'mtitem'), coords={'tid': tid_coord, 'mtitem': mtitem_coord})
        
        classifier_dts = xr.Dataset(
            data_vars=dict(
                raw=raw_var,
                rmeta=rmeta_var,
                hmeta=hmeta_var,
                tmeta=tmeta_var),
            coords=dict(
                hmer=('hmer', hmer_coord),
                rid=('rid', rid_coord),
                tid=('tid', tid_coord),
                mritem=('mritem', mritem_coord),
                mtitem=('mtitem', mtitem_coord),
                mhitem=('mhitem', mhitem_coord)),
            attrs=dict(
                cls_name=name,
                cls_type=cls.MODEL_NAME,
                cls_created=current_timestamp.strftime(cls.DATETIME_FORMAT),
                cls_author=author,
                cls_database_name=db_name,
                cls_database_type=db_type,
                cls_kmer_sizes="|".join([str(ksize) for ksize in kmer_sizes]),
                cls_description="Classifier Type: {}. Database {}. Kmer Sizes: {}".format(cls.MODEL_NAME, db_name, ",".join([str(ksize) for ksize in kmer_sizes])),
                cls_compression=str(compress)))
        
        print("> Creating final storage file.")
        
        delayed_save = classifier_dts.to_netcdf(
            path=classifier_fp,
            format='NETCDF4',
            engine='h5netcdf',
            unlimited_dims={'mritem','mtitem'},
            compute=False,
            encoding=dict(
                raw=dict(dtype='b', chunksizes=raw_chunksize, compression='lzf', shuffle=True),
                rmeta=dict(dtype='u4', chunksizes=rmeta_chunksize, compression='lzf', shuffle=True),
                tmeta=dict(dtype='u4', chunksizes=tmeta_chunksize, compression='lzf', shuffle=True),
                hmeta=dict(dtype='u2', chunksizes=hmeta_chunksize, zlib=True, complevel=cmp_level, shuffle=True)))
        
        with ProgressBar():
            delayed_save.compute(scheduler='threads', num_workers=nworkers[1], optimize_graph=True)
        
        # closing active files
        database_instance.close()
        classifier_dts.close()
        parser_memory_cache.clear()
        if cache_dir_fp is None:
            tmp_dir.cleanup()
        return

    def classify(self, dataset, **kwargs):
        if not self.verify_dataset(dataset):
            raise TypeError('`dataset_instance` has invalid type.')
        if set(self.kmer_sizes) != set(dataset.kmer_sizes):
            raise TypeError('Dataset and classifier have different kmers.')
    
    @property
    def xtid(self):
        return self._index_tid

    @property
    def xmtitem(self):
        return self._index_mtitem

    @property
    def tmeta(self):
        if self.state:
            if self.chunked:
                return self.tmeta
            else:
                return self.xr['tmeta']
        else:
            raise RuntimeError('Basefile is closed or at hold.')

    @property
    def database(self):
        return self._db_instance